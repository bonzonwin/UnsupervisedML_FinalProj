{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category                                            Message\n",
      "0      ham  jurong point crazy available bugis n great wor...\n",
      "1      ham                            ok lar joking wif u oni\n",
      "2     spam  free entry 2 wkly comp win fa cup final tkts 2...\n",
      "3      ham                        u dun say early hor u c say\n",
      "4      ham                      nah dont think goes usf lives\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in ENGLISH_STOP_WORDS)\n",
    "    return text\n",
    "\n",
    "# Define the remove stop words function\n",
    "def remove_stop_words(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply the cleaning functions\n",
    "data['Message'] = data['Message'].apply(clean_text).apply(remove_stop_words)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['Message']\n",
    "y = data['Category'].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   020603  0207  02073162414  020903  0578  071104  07123456789  07734396839  \\\n",
      "0     0.0   0.0          0.0     0.0   0.0     0.0          0.0          0.0   \n",
      "1     0.0   0.0          0.0     0.0   0.0     0.0          0.0          0.0   \n",
      "2     0.0   0.0          0.0     0.0   0.0     0.0          0.0          0.0   \n",
      "3     0.0   0.0          0.0     0.0   0.0     0.0          0.0          0.0   \n",
      "4     0.0   0.0          0.0     0.0   0.0     0.0          0.0          0.0   \n",
      "\n",
      "   0776xxxxxxx  07786200117  ...  youve   yr  yrs  yummy  yun  yuo  yup  zed  \\\n",
      "0          0.0          0.0  ...    0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "1          0.0          0.0  ...    0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "2          0.0          0.0  ...    0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "3          0.0          0.0  ...    0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "4          0.0          0.0  ...    0.0  0.0  0.0    0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   zoe  Ã¼ll  \n",
      "0  0.0  0.0  \n",
      "1  0.0  0.0  \n",
      "2  0.0  0.0  \n",
      "3  0.0  0.0  \n",
      "4  0.0  0.0  \n",
      "\n",
      "[5 rows x 3000 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns=feature_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "held       0.389274\n",
      "87239      0.365328\n",
      "cup        0.349554\n",
      "weekly     0.293788\n",
      "world      0.284566\n",
      "end        0.281332\n",
      "100        0.275464\n",
      "service    0.261294\n",
      "win        0.252072\n",
      "reply      0.219999\n",
      "stop       0.219999\n",
      "send       0.204650\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Analyzing Non-zero Features\n",
    "document_vector = tfidf_df.iloc[0]\n",
    "non_zero_features = document_vector[document_vector > 0]\n",
    "print(non_zero_features.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS118B_WI24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
